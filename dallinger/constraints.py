# /// script
# dependencies = ["click", "requests"]
# ///
#
# You can run this script directly without installing Dallinger,
# for example:
#    uv run generate_constraints.py --update-existing
# or
#    curl -s https://raw.githubusercontent.com/Dallinger/Dallinger/generate-constraints/dallinger/constraints.py | uv run -

import contextlib
import logging
import os
import re
import subprocess
import tempfile
from hashlib import md5
from pathlib import Path
from typing import Optional

import click
import requests

logger = logging.getLogger(__name__)


@click.group()
def constraints_cli():
    """Dallinger constraints file utilities."""


@constraints_cli.command()
@click.argument("directory", type=click.Path(exists=True, file_okay=False))
def check(directory: str):
    """
    Check whether a constraints.txt file exists and is up to date for the specified directory.
    """
    check_constraints(directory)


def check_constraints(directory: str):
    """
    Check whether a constraints.txt file exists and is up to date for the specified directory.
    """
    constraints_path = Path(directory) / "constraints.txt"
    if not constraints_path.exists():
        raise FileNotFoundError(f"constraints.txt file not found in {directory}")
    input_path = _find_input_path(directory)
    if not _constraints_up_to_date(constraints_path, input_path):
        raise ValueError(
            f"constraints.txt file in {directory} is not up to date with {input_path.name}."
        )
    logger.info(
        f"constraints.txt file in {directory} exists and is up to date with {input_path.name}."
    )


@constraints_cli.command()
@click.argument("directory", type=click.Path(exists=True, file_okay=False))
def generate(directory: str):
    """
    Generate a constraints.txt file for the specified directory.
    """
    generate_constraints(directory)


def generate_constraints(directory: str):
    """
    Generate a constraints.txt file for the specified directory.

    The command looks for an input file in the specified directory,
    either requirements.txt or pyproject.toml.
    If neither are present, a requirements.txt file is created with
    dallinger as its only dependency.

    This input file is processed to identify the requested version of Dallinger.
    The corresponding dev-requirements.txt file for that version of Dallinger is then
    sourced from the Dallinger GitHub repository. This file specifies pinned versions
    for all Dallinger dependencies, validated in the CI.

    The constraints.txt file is then generated by reference to both the input file
    and the dev-requirements.txt file. It contains a list of versions for all dependencies
    (explicit and implicit).

    This generation process uses ``uv pip-compile`` if uv is available, otherwise the slower
    legacy ``pip-compile`` is used.

    If the constraints.txt file exists already it will be overwritten.
    For a version of this function that does not automatically overwrite the constraints.txt file,
    see ``ensure_constraints_file_presence``.

    Parameters
    ----------
    directory : str
        That path to the directory for which a constraints.txt file is to be generated.
    """
    input_path = _find_input_path(directory)
    output_path = Path(directory) / "constraints.txt"

    dallinger_reference = _get_dallinger_reference(input_path)
    dallinger_dev_requirements_path = _get_dallinger_dev_requirements_path(
        dallinger_reference
    )
    _test_dallinger_dev_requirements_path(dallinger_dev_requirements_path)

    print(
        f"Compiling constraints.txt file from {input_path} and {dallinger_dev_requirements_path}"
    )
    compile_info = f"dallinger generate-constraints\n#\n# Compiled from a {Path(input_path).name} file with md5sum: {_hash_input_file(input_path)}"

    _pip_compile(
        input_path,
        output_path,
        constraints=[dallinger_dev_requirements_path],
        compile_info=compile_info,
    )

    _make_paths_relative(output_path)


@constraints_cli.command()
@click.argument("directory", type=click.Path(exists=True, file_okay=False))
def ensure(directory: str):
    """
    Ensure that a constraints.txt file exists for the specified directory,
    preserving the existing constraints.txt file if it exists and is up to date,
    and updating it if it is out of date.
    """
    ensure_constraints_file_presence(directory)


def ensure_constraints_file_presence(directory: str):
    """
    Ensures that a ``constraints.txt`` file exists in the specified directory.

    - If the environment variable SKIP_DEPENDENCY_CHECK is set, no action will be performed.
    - If no ``constraints.txt`` file exists, one will be automatically generated using.
    - If a manually written ``constraints.txt`` file exists already, then no action will be taken.
    - If an automatically generated ``constraints.txt`` file exists already,
      but it seems up-to-date with the input file (i.e. the MD5 hash of the input file is present in the constraints.txt file),
      then no action will be taken.
    - If an automatically generated ``constraints.txt`` file exists already, but it seems out-of-date,
      then ``constraints.txt`` will be automatically updated.
    """
    if os.environ.get("SKIP_DEPENDENCY_CHECK"):
        return

    input_path = _find_input_path(directory)
    output_path = Path(directory) / "constraints.txt"

    if output_path.exists():
        if not _constraints_autogenerated(output_path):
            logger.info(
                "%s was written manually, no attempt will be made to update it.",
                output_path,
            )
            return
        if _constraints_up_to_date(output_path, input_path):
            logger.info("%s is up to date with %s.", output_path, input_path)
            return
        logger.info("%s is out of date with %s, updating.", output_path, input_path)
    generate_constraints(directory)


def _find_input_path(directory: str) -> Path:
    requirements_path = Path(directory) / "requirements.txt"
    pyproject_path = Path(directory) / "pyproject.toml"

    if requirements_path.exists():
        input_path = requirements_path
    elif pyproject_path.exists():
        input_path = pyproject_path
    else:
        logger.warning(
            "No requirements.txt or pyproject.toml file found, will autogenerate a requirements.txt"
        )
        requirements_path.write_text("dallinger\n")
        input_path = requirements_path

    return input_path


def _hash_input_file(input_path: Path) -> str:
    with open(input_path, "rb") as f:
        return md5(f.read()).hexdigest()


def _constraints_autogenerated(constraints_path: Path) -> bool:
    text = constraints_path.read_text()
    return bool(re.search(r"This file (is|was) autogenerated", text))


def _constraints_up_to_date(constraints_path: Path, input_path: Path) -> bool:
    return _hash_input_file(input_path) in constraints_path.read_text()


def _get_dallinger_reference(input_path: Path) -> str:
    explicit_reference = _get_explicit_dallinger_reference(input_path)
    if explicit_reference:
        return explicit_reference
    else:
        return _get_implied_dallinger_reference(input_path)


def _get_explicit_dallinger_reference(input_path: Path) -> Optional[str]:
    release = _get_explicit_dallinger_numbered_release(input_path)
    if release:
        return f"v{release}"
    else:
        return _get_explicit_dallinger_github_requirement(input_path)


def _get_explicit_dallinger_numbered_release(input_path: Path) -> Optional[str]:
    # Should catch patterns like dallinger[docker,test]==11.5.0
    pattern = re.compile(r"dallinger(?:\[[^\]]+\])?==([0-9]+\.[0-9]+\.[0-9]+)")
    with open(input_path, "r") as f:
        for line in f:
            match = pattern.search(line)
            if match:
                return match.group(1)
    return None


def _get_explicit_dallinger_github_requirement(input_path: Path) -> Optional[str]:
    # dallinger@git+https://github.com/Dallinger/Dallinger.git@my-branch#egg=dallinger
    pattern = re.compile(
        r"dallinger\s*@\s*git\+https://github\.com/Dallinger/Dallinger(?:\.git)?@([^\s#]+)(?:#.*)?"
    )
    with open(input_path, "r") as f:
        for line in f:
            match = pattern.search(line)
            if match:
                return match.group(1)
    return None


def _get_implied_dallinger_reference(input_path: Path) -> str:
    with tempfile.NamedTemporaryFile(suffix=".txt") as tmpfile:
        _pip_compile(input_path, tmpfile.name, constraints=None)
        retrieved = _get_explicit_dallinger_reference(Path(tmpfile.name))
        if retrieved is None:
            raise ValueError(
                f"Failed to retrieve an implied Dallinger reference from {input_path}. "
                "Consider specifying Dallinger explicitly in the requirements.txt file."
            )
    return retrieved


def _get_dallinger_dev_requirements_path(dallinger_reference: str) -> str:
    return f"https://raw.githubusercontent.com/Dallinger/Dallinger/{dallinger_reference}/dev-requirements.txt"


def _test_dallinger_dev_requirements_path(url: str):
    try:
        response = requests.get(url, timeout=10)
    except requests.exceptions.ConnectionError as e:
        raise RuntimeError(
            """It looks like you're offline. Dallinger can't generate constraints
To get a valid constraints.txt file you can copy the requirements.txt file:
cp requirements.txt constraints.txt"""
        ) from e
    if response.status_code != 200:
        raise ValueError(
            f"{url} not found. Please make sure your specified Dallinger "
            "version exists in the Dallinger repository. "
        )


def _pip_compile(
    in_file, out_file, constraints: Optional[list] = None, compile_info=None
):
    use_uv = uv_available()
    if use_uv:
        logger.info("Calling `uv pip-compile`...")
        cmd = ["uv", "pip", "compile"]
    else:
        logger.info(
            "Calling `pip-compile` (consider installing uv for faster compilation)..."
        )
        cmd = ["pip-compile"]
    cmd += [
        str(in_file),
        "--output-file",
        str(out_file),
    ]
    if constraints:
        for constraint in constraints:
            cmd += ["--constraint", constraint]

    env = dict(os.environ)
    if compile_info:
        if use_uv:
            env["UV_CUSTOM_COMPILE_COMMAND"] = compile_info
        else:
            env["CUSTOM_COMPILE_COMMAND"] = compile_info
    subprocess.check_output(
        cmd,
        env=env,
    )


def uv_available() -> bool:
    """
    Check whether uv is available for use.
    """
    try:
        subprocess.check_output(["uv", "--version"])
        return True
    except subprocess.CalledProcessError:
        return False


def _make_paths_relative(constraints_path: Path):
    constraints_contents = constraints_path.read_text()
    constraints_contents_amended = re.sub(
        "via -r .*requirements.txt", "via -r requirements.txt", constraints_contents
    )
    constraints_path.write_text(constraints_contents_amended)


@contextlib.contextmanager
def working_directory(path):
    start_dir = os.getcwd()
    try:
        os.chdir(path)
        yield
    finally:
        os.chdir(start_dir)


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(message)s",
    )
    constraints_cli()
