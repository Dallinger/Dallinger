# /// script
# dependencies = ["requests"]
# ///
#
# You can run this script directly without installing Dallinger,
# for example:
#    uv run generate_constraints.py --update-existing
# or
#    curl -s https://raw.githubusercontent.com/Dallinger/Dallinger/generate-constraints/dallinger/generate_constraints.py | uv run -

import argparse
import contextlib
import logging
import os
import re
import subprocess
import tempfile
from hashlib import md5
from pathlib import Path
from typing import Optional

import requests

logger = logging.getLogger(__name__)


def generate_constraints(directory: str):
    """
    Generate a constraints.txt file for the specified directory.

    The command looks for an input file in the specified directory,
    either requirements.txt or pyproject.toml.
    If neither are present, a requirements.txt file is created with
    dallinger as its only dependency.

    This input file is processed to identify the requested version of Dallinger.
    The corresponding dev-requirements.txt file for that version of Dallinger is then
    sourced from the Dallinger GitHub repository. This file specifies pinned versions
    for all Dallinger dependencies, validated in the CI.

    The constraints.txt file is then generated by reference to both the input file
    and the dev-requirements.txt file. It contains a list of versions for all dependencies
    (explicit and implicit).

    This generation process uses ``uv pip-compile`` if uv is available, otherwise the slower
    legacy ``pip-compile`` is used.

    If the constraints.txt file exists already it will be overwritten.
    For a version of this function that does not automatically overwrite the constraints.txt file,
    see ``ensure_constraints_file_presence``.

    Parameters
    ----------
    directory : str
        That path to the directory for which a constraints.txt file is to be generated.
    """
    input_path = _find_input_path(directory)
    output_path = Path(directory) / "constraints.txt"

    dallinger_reference = _get_dallinger_reference(input_path)
    dallinger_dev_requirements_path = _get_dallinger_dev_requirements_path(
        dallinger_reference
    )
    _test_dallinger_dev_requirements_path(dallinger_dev_requirements_path)

    print(
        f"Compiling constraints.txt file from {input_path} and {dallinger_dev_requirements_path}"
    )
    compile_info = f"dallinger generate-constraints\n#\n# Compiled from a {Path(input_path).name} file with md5sum: {_hash_input_file(input_path)}"

    _pip_compile(
        input_path,
        output_path,
        constraints=[dallinger_dev_requirements_path],
        compile_info=compile_info,
    )

    _make_paths_relative(output_path)


def ensure_constraints_file_presence(directory: str, update_existing: bool = False):
    """
    Ensures that a ``constraints.txt`` file exists in the specified directory.

    - If the environment variable SKIP_DEPENDENCY_CHECK is set, no action will be performed.
    - If no ``constraints.txt`` file exists, one will be automaticallygenerated using ``generate_constraints``.
    - If a manually written ``constraints.txt`` file exists already, then no action will be taken.
    - If an automatically generated ``constraints.txt`` file exists already,
      but it seems up-to-date with the input file (i.e. the MD5 hash of the input file is present in the constraints.txt file),
      then no action will be taken.
    - If an automatically generated ``constraints.txt`` file exists already, but it seems out-of-date,
      then a ``ValueError`` will be raised, unless ``update_existing`` is ``True`,
      in which case the ``constraints.txt`` file will be automatically updated.
    """
    if os.environ.get("SKIP_DEPENDENCY_CHECK"):
        return

    input_path = _find_input_path(directory)
    output_path = Path(directory) / "constraints.txt"

    if output_path.exists():
        if not _constraints_autogenerated(output_path):
            logger.info(
                "%s was written manually, no attempt will be made to update it.",
                output_path,
            )
            return
        if _constraints_up_to_date(output_path, input_path):
            logger.info("%s is up to date with %s.", output_path, input_path)
            return
        if update_existing:
            logger.info("%s is out of date with %s, updating.", output_path, input_path)
            generate_constraints(directory)
            return
        raise ValueError(
            "\nChanges detected to requirements.txt: run the command\n    dallinger generate-constraints\nand retry"
        )
    generate_constraints(directory)


def _find_input_path(directory: str) -> Path:
    requirements_path = Path(directory) / "requirements.txt"
    pyproject_path = Path(directory) / "pyproject.toml"

    if requirements_path.exists():
        input_path = requirements_path
    elif pyproject_path.exists():
        input_path = pyproject_path
    else:
        logger.warning(
            "No requirements.txt or pyproject.toml file found, will autogenerate a requirements.txt"
        )
        requirements_path.write_text("dallinger\n")
        input_path = requirements_path

    return input_path


def _hash_input_file(input_path: Path) -> str:
    with open(input_path, "rb") as f:
        return md5(f.read()).hexdigest()


def _constraints_autogenerated(constraints_path: Path) -> bool:
    text = constraints_path.read_text()
    return bool(re.search(r"This file (is|was) autogenerated", text))


def _constraints_up_to_date(constraints_path: Path, input_path: Path) -> bool:
    return _hash_input_file(input_path) in constraints_path.read_text()


def _get_dallinger_reference(input_path: Path) -> str:
    explicit_reference = _get_explicit_dallinger_reference(input_path)
    if explicit_reference:
        return explicit_reference
    else:
        return _get_implied_dallinger_reference(input_path)


def _get_explicit_dallinger_reference(input_path: Path) -> str | None:
    release = _get_explicit_dallinger_numbered_release(input_path)
    if release:
        return f"v{release}"
    else:
        return _get_explicit_dallinger_github_requirement(input_path)


def _get_explicit_dallinger_numbered_release(input_path: Path) -> str | None:
    # Should catch patterns like dallinger[docker,test]==11.5.0
    pattern = re.compile(r"dallinger(?:\[[^\]]+\])?==([0-9]+\.[0-9]+\.[0-9]+)")
    with open(input_path, "r") as f:
        for line in f:
            match = pattern.search(line)
            if match:
                return match.group(1)
    return None


def _get_explicit_dallinger_github_requirement(input_path: Path) -> str | None:
    # dallinger@git+https://github.com/Dallinger/Dallinger.git@my-branch#egg=dallinger
    pattern = re.compile(
        r"dallinger\s*@\s*git\+https://github\.com/Dallinger/Dallinger(?:\.git)?@([^\s#]+)(?:#.*)?"
    )
    with open(input_path, "r") as f:
        for line in f:
            match = pattern.search(line)
            if match:
                return match.group(1)
    return None


def _get_implied_dallinger_reference(input_path: Path) -> str:
    with tempfile.NamedTemporaryFile(suffix=".txt") as tmpfile:
        _pip_compile(input_path, tmpfile.name, constraints=None)
        retrieved = _get_explicit_dallinger_reference(Path(tmpfile.name))
        if retrieved is None:
            raise ValueError(
                f"Failed to retrieve an implied Dallinger reference from {input_path}. "
                "Consider specifying Dallinger explicitly in the requirements.txt file."
            )
    return retrieved


def _get_dallinger_dev_requirements_path(dallinger_reference: str) -> str:
    return f"https://raw.githubusercontent.com/Dallinger/Dallinger/{dallinger_reference}/dev-requirements.txt"


def _test_dallinger_dev_requirements_path(url: str):
    try:
        response = requests.get(url, timeout=10)
    except requests.exceptions.ConnectionError as e:
        raise RuntimeError(
            """It looks like you're offline. Dallinger can't generate constraints
To get a valid constraints.txt file you can copy the requirements.txt file:
cp requirements.txt constraints.txt"""
        ) from e
    if response.status_code != 200:
        raise ValueError(
            f"{url} not found. Please make sure your specified Dallinger "
            "version exists in the Dallinger repository. "
        )


def _pip_compile(
    in_file, out_file, constraints: Optional[list] = None, compile_info=None
):
    use_uv = uv_available()
    if use_uv:
        logger.info("Calling `uv pip-compile`...")
        cmd = ["uv", "pip", "compile"]
    else:
        logger.info(
            "Calling `pip-compile` (consider installing uv for faster compilation)..."
        )
        cmd = ["pip-compile"]
    cmd += [
        # "--verbose",
        str(in_file),
        "--output-file",
        str(out_file),
    ]
    if constraints:
        for constraint in constraints:
            cmd += ["--constraint", constraint]

    env = dict(os.environ)
    if compile_info:
        if use_uv:
            env["UV_CUSTOM_COMPILE_COMMAND"] = compile_info
        else:
            env["CUSTOM_COMPILE_COMMAND"] = compile_info
    subprocess.check_output(
        cmd,
        env=env,
    )


def uv_available() -> bool:
    """
    Check whether uv is available for use.
    """
    # return False
    try:
        subprocess.check_output(["uv", "--version"])
        return True
    except subprocess.CalledProcessError:
        return False


def _make_paths_relative(constraints_path: Path):
    constraints_contents = constraints_path.read_text()
    constraints_contents_amended = re.sub(
        "via -r .*requirements.txt", "via -r requirements.txt", constraints_contents
    )
    constraints_path.write_text(constraints_contents_amended)


@contextlib.contextmanager
def working_directory(path):
    start_dir = os.getcwd()
    try:
        os.chdir(path)
        yield
    finally:
        os.chdir(start_dir)


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(message)s",
    )
    parser = argparse.ArgumentParser(
        description="Generate or update a constraints.txt file for the current directory."
    )
    parser.add_argument(
        "--update-existing",
        action="store_true",
        help="Update the constraints.txt file if it already exists and is out of date.",
    )
    args = parser.parse_args()
    ensure_constraints_file_presence(Path.cwd(), update_existing=args.update_existing)
